{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f864190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os.path\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import time\n",
    "print('Packages loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe1d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_maf_re = re.compile(r'\\bMAF=([^;]+)')\n",
    "\n",
    "def extract_maf_regex(info_str):\n",
    "    \"\"\"\n",
    "    Use a regular expression to grab the MAF= value.\n",
    "    \"\"\"\n",
    "    m = _maf_re.search(info_str)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(1))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def classify_maf(maf):\n",
    "    if maf < 0.001:\n",
    "        return 'rare'\n",
    "    elif maf > 0.05:\n",
    "        return 'common'\n",
    "    else:\n",
    "        return 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c8089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inPath = '/scratch/ml-csm/datasets/genomics/ref-genome/human/GRCh38/ensembl/variants/processed/'\n",
    "df_maf = pd.read_parquet(inPath+'1000GENOMES-release114-maf.parquet.gz')\n",
    "\n",
    "df_maf['category'] = df_maf['maf'].apply(classify_maf)\n",
    "df_maf.dropna(inplace=True, ignore_index=True)\n",
    "df_maf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f05274",
   "metadata": {},
   "source": [
    "## Predictions 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0772f5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group = 1\n",
    "inPath = f'/home/sdodl001/Desktop/DNA_Methylation_Scripts/cpg_util_scripts/data/kmeans/uncert_gve_direction/{group}/pred200_merged/'\n",
    "files = os.listdir(inPath)\n",
    "files.sort()\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f90e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/148 files. Current unique names: 3645640\n",
      "Processed 20/148 files. Current unique names: 4195255\n",
      "Processed 30/148 files. Current unique names: 4554064\n",
      "Processed 40/148 files. Current unique names: 4695465\n",
      "Processed 50/148 files. Current unique names: 4931860\n",
      "Processed 60/148 files. Current unique names: 5099470\n",
      "Processed 70/148 files. Current unique names: 5138015\n",
      "Processed 80/148 files. Current unique names: 5149679\n",
      "Processed 90/148 files. Current unique names: 5257657\n",
      "Processed 100/148 files. Current unique names: 7958108\n",
      "Processed 110/148 files. Current unique names: 8837202\n",
      "Processed 120/148 files. Current unique names: 9015023\n",
      "Processed 130/148 files. Current unique names: 9120615\n",
      "Processed 140/148 files. Current unique names: 10246896\n",
      "Processed 148/148 files. Current unique names: 10358075\n",
      "\n",
      "Total unique names collected: 10358075\n",
      "Memory used by unique_names set: 114.34 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rs567865040', 'rs1426319946', 'rs927581744', 'rs1038629972', 'rs928388916']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty set to store unique name values\n",
    "unique_names = set()\n",
    "\n",
    "# Track progress\n",
    "total_files = len(files)\n",
    "processed = 0\n",
    "\n",
    "# Process files in batches to avoid memory issues\n",
    "for file in files:\n",
    "    try:\n",
    "        # Only load the 'name' column to minimize memory usage\n",
    "        data = pd.read_parquet(inPath + file, columns=['name'])\n",
    "        \n",
    "        # Update the set with new unique values\n",
    "        unique_names.update(data['name'])\n",
    "        \n",
    "        # Update and display progress\n",
    "        processed += 1\n",
    "        if processed % 10 == 0 or processed == total_files:\n",
    "            print(f\"Processed {processed}/{total_files} files. Current unique names: {len(unique_names)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nTotal unique names collected: {len(unique_names)}\")\n",
    "print(f\"Memory used by unique_names set: {sum(len(name) for name in unique_names) / (1024*1024):.2f} MB\")\n",
    "\n",
    "\n",
    "# Save the unique_names to a file\n",
    "\n",
    "# Create outputs directory if it doesn't exist (this won't be tracked by Git)\n",
    "import os\n",
    "output_dir = os.path.join(os.path.dirname(os.path.abspath('__file__')), 'outputs')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Saving large files to {output_dir} (not tracked by Git)\")\n",
    "\n",
    "# Save as pickle (Python's binary format - preserves the set data structure)\n",
    "output_path = os.path.join(output_dir, f'unique_names_group_{group}.pkl')\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(unique_names, f)\n",
    "print(f\"Saved unique names to: {output_path}\")\n",
    "print(\"WARNING: This file is large and should not be committed to Git.\")\n",
    "\n",
    "# # Optionally save as text file (one name per line)\n",
    "# text_output_path = os.path.join(output_dir, f'unique_names_group_{group}.txt')\n",
    "# with open(text_output_path, 'w') as f:\n",
    "#     for name in sorted(unique_names):\n",
    "#         f.write(f\"{name}\\n\")\n",
    "# print(f\"Saved unique names to: {text_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d65b45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3182456c",
   "metadata": {},
   "source": [
    "# Loading Saved Unique Names\n",
    "\n",
    "If you have previously saved the unique names to a file, you can load them instead of recreating the set. This is useful for:\n",
    "- Separating the data collection and analysis steps\n",
    "- Rerunning analyses with the same variant set\n",
    "- Sharing variant sets between different analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1565e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this section if you want to load previously saved unique names\n",
    "group = 1\n",
    "# Path to the saved unique names file\n",
    "unique_names_file = f'unique_names_group_{group}.pkl'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.isfile(unique_names_file):\n",
    "    # Load unique names from pickle file\n",
    "    with open(unique_names_file, 'rb') as f:\n",
    "        unique_names = pickle.load(f)\n",
    "    print(f\"Loaded {len(unique_names)} unique names from {unique_names_file}\")\n",
    "    print(f\"Memory used by unique_names set: {sum(len(name) for name in unique_names) / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    # Preview some unique names\n",
    "    print(\"\\nSample names:\")\n",
    "    print(list(unique_names)[:5])\n",
    "else:\n",
    "    print(f\"File {unique_names_file} not found. Please run the cell that creates and saves unique_names first.\")\n",
    "\n",
    "\n",
    "# Alternatively, you can load from text file if you don't have the pickle file\n",
    "'''\n",
    "unique_names_file = f'unique_names_group_{group}.txt'\n",
    "\n",
    "if os.path.isfile(unique_names_file):\n",
    "    # Load unique names from text file\n",
    "    unique_names = set()\n",
    "    with open(unique_names_file, 'r') as f:\n",
    "        for line in f:\n",
    "            unique_names.add(line.strip())\n",
    "            \n",
    "    print(f\"Loaded {len(unique_names)} unique names from {unique_names_file}\")\n",
    "else:\n",
    "    print(f\"File {unique_names_file} not found. Please run the cell that creates and saves unique_names first.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbbd3c4",
   "metadata": {},
   "source": [
    "# Creating a Binary Membership Matrix\n",
    "\n",
    "Now we'll create a binary matrix where:\n",
    "- Each row corresponds to a unique variant name\n",
    "- Each column corresponds to a file\n",
    "- Cell values are 1 (variant present in file) or 0 (variant absent)\n",
    "\n",
    "This implementation uses:\n",
    "1. A dictionary for fast lookups of variant indices\n",
    "2. Sparse matrix construction for memory efficiency\n",
    "3. Batch processing to manage memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f842838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of variant names to row indices\n",
    "start_time = time.time()\n",
    "name_to_idx = {name: idx for idx, name in enumerate(unique_names)}\n",
    "print(f\"Created name to index mapping in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Initialize lists to store the sparse matrix coordinates and values\n",
    "row_indices = []\n",
    "col_indices = []\n",
    "data_values = []\n",
    "\n",
    "# Process files to build the membership matrix\n",
    "start_time = time.time()\n",
    "total_files = len(files)\n",
    "processed = 0\n",
    "\n",
    "for file_idx, file in enumerate(files):\n",
    "    try:\n",
    "        # Only load the 'name' column\n",
    "        file_data = pd.read_parquet(inPath + file, columns=['name'])\n",
    "        \n",
    "        # Get unique names in this file (we only need each name once per file)\n",
    "        file_names = set(file_data['name'])\n",
    "        \n",
    "        # For each name in this file, add a 1 to the matrix\n",
    "        for name in file_names:\n",
    "            if name in name_to_idx:  # This should always be true but checking to be safe\n",
    "                row_indices.append(name_to_idx[name])\n",
    "                col_indices.append(file_idx)\n",
    "                data_values.append(1)\n",
    "        \n",
    "        # Update progress\n",
    "        processed += 1\n",
    "        if processed % 10 == 0 or processed == total_files:\n",
    "            print(f\"Processed {processed}/{total_files} files for matrix construction\")\n",
    "            print(f\"Current non-zero elements: {len(data_values)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file} for matrix: {str(e)}\")\n",
    "\n",
    "# Create a sparse matrix in CSR format (efficient for row operations)\n",
    "num_variants = len(unique_names)\n",
    "num_files = len(files)\n",
    "\n",
    "membership_matrix = sparse.csr_matrix(\n",
    "    (data_values, (row_indices, col_indices)),\n",
    "    shape=(num_variants, num_files)\n",
    ")\n",
    "\n",
    "print(f\"\\nMatrix shape: {membership_matrix.shape} (variants × files)\")\n",
    "print(f\"Number of non-zero elements: {membership_matrix.count_nonzero()} (variant occurrences)\")\n",
    "print(f\"Sparsity: {100 - 100 * membership_matrix.count_nonzero() / (num_variants * num_files):.2f}%\")\n",
    "print(f\"Memory usage: {membership_matrix.data.nbytes / 1024**2:.2f} MB (data)\")\n",
    "print(f\"Total construction time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66603c2e",
   "metadata": {},
   "source": [
    "# Analyzing the Membership Matrix\n",
    "\n",
    "With the membership matrix constructed, you can perform various analyses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570641f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Count variants per file (column sums)\n",
    "file_variant_counts = membership_matrix.sum(axis=0).A1  # A1 converts to 1D array\n",
    "\n",
    "# Show files with most and least variants\n",
    "print(f\"Average variants per file: {file_variant_counts.mean():.2f}\")\n",
    "print(f\"Max variants in a file: {file_variant_counts.max()} (file #{file_variant_counts.argmax()}: {files[file_variant_counts.argmax()]})\")\n",
    "print(f\"Min variants in a file: {file_variant_counts.min()} (file #{file_variant_counts.argmin()}: {files[file_variant_counts.argmin()]})\")\n",
    "\n",
    "# Example 2: Count how many files each variant appears in (row sums)\n",
    "variant_file_counts = membership_matrix.sum(axis=1).A1\n",
    "\n",
    "# Show distribution of variant occurrence\n",
    "from collections import Counter\n",
    "occurrence_dist = Counter(variant_file_counts)\n",
    "\n",
    "# Find variants that appear in exactly 1 file\n",
    "min_file_threshold = 1\n",
    "cell_specific_variants = np.where(variant_file_counts == min_file_threshold)[0]\n",
    "print(f\"\\nNumber of variants appearing in exactly {min_file_threshold} file: {len(cell_specific_variants)}\")\n",
    "\n",
    "# Calculate percentage of these variants relative to all variants\n",
    "percentage = 100 * len(cell_specific_variants) / len(variant_file_counts)\n",
    "print(f\"This represents {percentage:.2f}% of all variants\")\n",
    "\n",
    "idx_to_name = {idx: name for name, idx in name_to_idx.items()}\n",
    "cell_specific_variant_names150 = [idx_to_name[idx] for idx in cell_specific_variants]  \n",
    "print(f\"Cell specific variants: {len(cell_specific_variant_names150)}\")\n",
    "\n",
    "\n",
    "# Print top 10 most common occurrence counts\n",
    "print(\"\\nVariant occurrence distribution:\")\n",
    "for count, num_variants in sorted(occurrence_dist.items())[:10]:\n",
    "    print(f\"{num_variants} variants appear in exactly {int(count)} files\")\n",
    "\n",
    "# Example 3: Find variants that appear in at least 80 files\n",
    "min_file_threshold = 50\n",
    "frequent_variants = np.where(variant_file_counts >= min_file_threshold)[0]\n",
    "print(f\"\\nNumber of variants appearing in at least {min_file_threshold} files: {len(frequent_variants)}\")\n",
    "# Calculate percentage of these variants relative to all variants\n",
    "percentage = 100 * len(frequent_variants) / len(variant_file_counts)\n",
    "print(f\"This represents {percentage:.2f}% of all variants\")\n",
    "\n",
    "# Example 4: Find variants that appear in at least 80 files\n",
    "min_file_threshold = 80\n",
    "cell_nonspecific_variants = np.where(variant_file_counts >= min_file_threshold)[0]\n",
    "print(f\"\\nNumber of variants appearing in at least {min_file_threshold} files: {len(cell_nonspecific_variants)}\")\n",
    "# Calculate percentage of these variants relative to all variants\n",
    "percentage = 100 * len(cell_nonspecific_variants) / len(variant_file_counts)\n",
    "print(f\"This represents {percentage:.2f}% of all variants\")\n",
    "\n",
    "idx_to_name = {idx: name for name, idx in name_to_idx.items()}\n",
    "cell_nonspecific_variant_names150 = [idx_to_name[idx] for idx in cell_nonspecific_variants]  \n",
    "print(f\"Cell non-specific variants: {len(cell_nonspecific_variant_names150)}\")\n",
    "\n",
    "# Example 4: Find variants that appear in all files\n",
    "universal_variants = np.where(variant_file_counts == len(files))[0]\n",
    "print(f\"\\nNumber of variants appearing in all {len(files)} files: {len(universal_variants)}\")\n",
    "\n",
    "# If you need to get the names of specific variants (e.g., universal ones)\n",
    "if len(universal_variants) > 0:\n",
    "    idx_to_name = {idx: name for name, idx in name_to_idx.items()}\n",
    "    universal_variant_names = [idx_to_name[idx] for idx in universal_variants[:5]]  # Show first 5\n",
    "    print(f\"Sample universal variants: {universal_variant_names}\")\n",
    "\n",
    "# Example 5: Save the matrix for future use (if needed)\n",
    "\n",
    "# # Save the sparse matrix\n",
    "# import pickle\n",
    "# with open(f'variant_membership_matrix_group_{group}.pkl', 'wb') as f:\n",
    "#     pickle.dump({\n",
    "#         'matrix': membership_matrix,\n",
    "#         'variant_names': list(name_to_idx.keys()), \n",
    "#         'file_names': files\n",
    "#     }, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565a5cd8",
   "metadata": {},
   "source": [
    "## Prediction 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8513df8",
   "metadata": {},
   "source": [
    "group = 1\n",
    "thr = 0.10\n",
    "\n",
    "inPath = f'/scratch/ml-csm/projects/fgenom/gve/output/kmeans/pred1/aggr/thr{thr}/{group}/'\n",
    "\n",
    "files = os.listdir(inPath)\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84595903",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize an empty set to store unique name values\n",
    "unique_names = set()\n",
    "\n",
    "# Track progress\n",
    "total_files = len(files)\n",
    "processed = 0\n",
    "\n",
    "# Process files in batches to avoid memory issues\n",
    "for file in files:\n",
    "    try:\n",
    "        # Only load the 'name' column to minimize memory usage\n",
    "        data = pd.read_parquet(inPath + file, columns=['name'])\n",
    "        \n",
    "        # Update the set with new unique values\n",
    "        unique_names.update(data['name'])\n",
    "        \n",
    "        # Update and display progress\n",
    "        processed += 1\n",
    "        if processed % 10 == 0 or processed == total_files:\n",
    "            print(f\"Processed {processed}/{total_files} files. Current unique names: {len(unique_names)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nTotal unique names collected: {len(unique_names)}\")\n",
    "print(f\"Memory used by unique_names set: {sum(len(name) for name in unique_names) / (1024*1024):.2f} MB\")\n",
    "\n",
    "\n",
    "# Save the unique_names to a file\n",
    "\n",
    "\n",
    "# Save as pickle (Python's binary format - preserves the set data structure)\n",
    "with open(f'unique_names_group_{group}_pred{1}.pkl', 'wb') as f:\n",
    "    pickle.dump(unique_names, f)\n",
    "print(f\"Saved unique names to: unique_names_group_{group}_pred{1}.pkl\")\n",
    "\n",
    "# # Optionally save as text file (one name per line)\n",
    "# with open(f'unique_names_group_{group}.txt', 'w') as f:\n",
    "#     for name in sorted(unique_names):\n",
    "#         f.write(f\"{name}\\n\")\n",
    "# print(f\"Saved unique names to: unique_names_group_{group}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fb8987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this section if you want to load previously saved unique names\n",
    "\n",
    "# Path to the saved unique names file\n",
    "unique_names_file = f'/home/sdodl001/UAVarPrior/uavarprior/interpret/unique_names_group_{group}_pred{1}.pkl'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.isfile(unique_names_file):\n",
    "    # Load unique names from pickle file\n",
    "    with open(unique_names_file, 'rb') as f:\n",
    "        unique_names = pickle.load(f)\n",
    "    print(f\"Loaded {len(unique_names)} unique names from {unique_names_file}\")\n",
    "    print(f\"Memory used by unique_names set: {sum(len(name) for name in unique_names) / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    # Preview some unique names\n",
    "    print(\"\\nSample names:\")\n",
    "    print(list(unique_names)[:5])\n",
    "else:\n",
    "    print(f\"File {unique_names_file} not found. Please run the cell that creates and saves unique_names first.\")\n",
    "\n",
    "\n",
    "# Alternatively, you can load from text file if you don't have the pickle file\n",
    "'''\n",
    "unique_names_file = f'unique_names_group_{group}.txt'\n",
    "\n",
    "if os.path.isfile(unique_names_file):\n",
    "    # Load unique names from text file\n",
    "    unique_names = set()\n",
    "    with open(unique_names_file, 'r') as f:\n",
    "        for line in f:\n",
    "            unique_names.add(line.strip())\n",
    "            \n",
    "    print(f\"Loaded {len(unique_names)} unique names from {unique_names_file}\")\n",
    "else:\n",
    "    print(f\"File {unique_names_file} not found. Please run the cell that creates and saves unique_names first.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f917ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of variant names to row indices\n",
    "start_time = time.time()\n",
    "name_to_idx = {name: idx for idx, name in enumerate(unique_names)}\n",
    "print(f\"Created name to index mapping in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Initialize lists to store the sparse matrix coordinates and values\n",
    "row_indices = []\n",
    "col_indices = []\n",
    "data_values = []\n",
    "\n",
    "# Process files to build the membership matrix\n",
    "start_time = time.time()\n",
    "total_files = len(files)\n",
    "processed = 0\n",
    "\n",
    "for file_idx, file in enumerate(files):\n",
    "    try:\n",
    "        # Only load the 'name' column\n",
    "        file_data = pd.read_parquet(inPath + file, columns=['name'])\n",
    "        \n",
    "        # Get unique names in this file (we only need each name once per file)\n",
    "        file_names = set(file_data['name'])\n",
    "        \n",
    "        # For each name in this file, add a 1 to the matrix\n",
    "        for name in file_names:\n",
    "            if name in name_to_idx:  # This should always be true but checking to be safe\n",
    "                row_indices.append(name_to_idx[name])\n",
    "                col_indices.append(file_idx)\n",
    "                data_values.append(1)\n",
    "        \n",
    "        # Update progress\n",
    "        processed += 1\n",
    "        if processed % 10 == 0 or processed == total_files:\n",
    "            print(f\"Processed {processed}/{total_files} files for matrix construction\")\n",
    "            print(f\"Current non-zero elements: {len(data_values)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file} for matrix: {str(e)}\")\n",
    "\n",
    "# Create a sparse matrix in CSR format (efficient for row operations)\n",
    "num_variants = len(unique_names)\n",
    "num_files = len(files)\n",
    "\n",
    "membership_matrix = sparse.csr_matrix(\n",
    "    (data_values, (row_indices, col_indices)),\n",
    "    shape=(num_variants, num_files)\n",
    ")\n",
    "\n",
    "print(f\"\\nMatrix shape: {membership_matrix.shape} (variants × files)\")\n",
    "print(f\"Number of non-zero elements: {membership_matrix.count_nonzero()} (variant occurrences)\")\n",
    "print(f\"Sparsity: {100 - 100 * membership_matrix.count_nonzero() / (num_variants * num_files):.2f}%\")\n",
    "print(f\"Memory usage: {membership_matrix.data.nbytes / 1024**2:.2f} MB (data)\")\n",
    "print(f\"Total construction time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d914a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Count variants per file (column sums)\n",
    "file_variant_counts = membership_matrix.sum(axis=0).A1  # A1 converts to 1D array\n",
    "\n",
    "# Show files with most and least variants\n",
    "print(f\"Average variants per file: {file_variant_counts.mean():.2f}\")\n",
    "print(f\"Max variants in a file: {file_variant_counts.max()} (file #{file_variant_counts.argmax()}: {files[file_variant_counts.argmax()]})\")\n",
    "print(f\"Min variants in a file: {file_variant_counts.min()} (file #{file_variant_counts.argmin()}: {files[file_variant_counts.argmin()]})\")\n",
    "\n",
    "# Example 2: Count how many files each variant appears in (row sums)\n",
    "variant_file_counts = membership_matrix.sum(axis=1).A1\n",
    "\n",
    "# Show distribution of variant occurrence\n",
    "from collections import Counter\n",
    "occurrence_dist = Counter(variant_file_counts)\n",
    "\n",
    "# Find variants that appear in exactly 1 file\n",
    "min_file_threshold = 1\n",
    "cell_specific_variants = np.where(variant_file_counts == min_file_threshold)[0]\n",
    "print(f\"\\nNumber of variants appearing in exactly {min_file_threshold} file: {len(cell_specific_variants)}\")\n",
    "\n",
    "# Calculate percentage of these variants relative to all variants\n",
    "percentage = 100 * len(cell_specific_variants) / len(variant_file_counts)\n",
    "print(f\"This represents {percentage:.2f}% of all variants\")\n",
    "\n",
    "idx_to_name = {idx: name for name, idx in name_to_idx.items()}\n",
    "cell_specific_variant_names1 = [idx_to_name[idx] for idx in cell_specific_variants]  \n",
    "print(f\"Cell specific variants: {len(cell_specific_variant_names1)}\")\n",
    "\n",
    "\n",
    "# Print top 10 most common occurrence counts\n",
    "print(\"\\nVariant occurrence distribution:\")\n",
    "for count, num_variants in sorted(occurrence_dist.items())[1:10]:\n",
    "    print(f\"{num_variants} variants appear in exactly {int(count)} files\")\n",
    "\n",
    "# Example 3: Find variants that appear in at least 80 files\n",
    "min_file_threshold = 50\n",
    "frequent_variants = np.where(variant_file_counts >= min_file_threshold)[0]\n",
    "print(f\"\\nNumber of variants appearing in at least {min_file_threshold} files: {len(frequent_variants)}\")\n",
    "# Calculate percentage of these variants relative to all variants\n",
    "percentage = 100 * len(frequent_variants) / len(variant_file_counts)\n",
    "print(f\"This represents {percentage:.2f}% of all variants\")\n",
    "\n",
    "# Example 4: Find variants that appear in at least 80 files\n",
    "min_file_threshold = 80\n",
    "cell_nonspecific_variants = np.where(variant_file_counts >= min_file_threshold)[0]\n",
    "print(f\"\\nNumber of variants appearing in at least {min_file_threshold} files: {len(cell_nonspecific_variants)}\")\n",
    "# Calculate percentage of these variants relative to all variants\n",
    "percentage = 100 * len(cell_nonspecific_variants) / len(variant_file_counts)\n",
    "print(f\"This represents {percentage:.2f}% of all variants\")\n",
    "\n",
    "idx_to_name = {idx: name for name, idx in name_to_idx.items()}\n",
    "cell_nonspecific_variant_names1 = [idx_to_name[idx] for idx in cell_nonspecific_variants]  \n",
    "print(f\"Cell non-specific variants: {len(cell_nonspecific_variant_names1)}\")\n",
    "\n",
    "# Example 5: Find variants that appear in all files\n",
    "universal_variants = np.where(variant_file_counts == len(files))[0]\n",
    "print(f\"\\nNumber of variants appearing in all {len(files)} files: {len(universal_variants)}\")\n",
    "\n",
    "# If you need to get the names of specific variants (e.g., universal ones)\n",
    "if len(universal_variants) > 0:\n",
    "    idx_to_name = {idx: name for name, idx in name_to_idx.items()}\n",
    "    universal_variant_names = [idx_to_name[idx] for idx in universal_variants[:5]]  # Show first 5\n",
    "    print(f\"Sample universal variants: {universal_variant_names}\")\n",
    "\n",
    "# Example 5: Save the matrix for future use (if needed)\n",
    "\n",
    "# # Save the sparse matrix\n",
    "# import pickle\n",
    "# with open(f'variant_membership_matrix_group_{group}_pred{1}.pkl', 'wb') as f:\n",
    "#     pickle.dump({\n",
    "#         'matrix': membership_matrix,\n",
    "#         'variant_names': list(name_to_idx.keys()), \n",
    "#         'file_names': files\n",
    "#     }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d73032",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = df_maf['id'].isin(cell_specific_variant_names1)\n",
    "df_maf_pred1 = df_maf[ind1]\n",
    "rare_count = (df_maf_pred1['category'] == 'rare').sum()\n",
    "common_count = (df_maf_pred1['category'] == 'common').sum()\n",
    "rare_count, common_count, len(df_maf_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb72bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = df_maf['id'].isin(cell_nonspecific_variant_names1)\n",
    "df_maf_pred1 = df_maf[ind1]\n",
    "rare_count = (df_maf_pred1['category'] == 'rare').sum()\n",
    "common_count = (df_maf_pred1['category'] == 'common').sum()\n",
    "rare_count, common_count, len(df_maf_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb90daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind150 = df_maf['id'].isin(cell_specific_variant_names150)\n",
    "df_maf_pred150 = df_maf[ind150]\n",
    "rare_count150 = (df_maf_pred150['category'] == 'rare').sum()\n",
    "common_count150 = (df_maf_pred150['category'] == 'common').sum()\n",
    "rare_count150, common_count150, len(df_maf_pred150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421d6b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind150 = df_maf['id'].isin(cell_nonspecific_variant_names150)\n",
    "df_maf_pred150 = df_maf[ind150]\n",
    "rare_count150 = (df_maf_pred150['category'] == 'rare').sum()\n",
    "common_count150 = (df_maf_pred150['category'] == 'common').sum()\n",
    "rare_count150, common_count150, len(df_maf_pred150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e455a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pred1 & Pred150: cell-specific & Cell non-specific: rare/common\n",
    "414426/67388, 193/19, 305524/49357, 112/8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10dcebe",
   "metadata": {},
   "source": [
    "## Save Cell-specific and Cell-nonspecific Variant Names\n",
    "\n",
    "Let's save the variant names we've identified for both prediction models to files for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf53750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "import os\n",
    "output_dir = os.path.join(os.path.dirname(os.path.abspath('__file__')), 'outputs')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save cell-specific variant names for prediction model 1\n",
    "pred1_cell_specific_output = os.path.join(output_dir, f'cell_specific_variants_pred1_group_{group}.pkl')\n",
    "with open(pred1_cell_specific_output, 'wb') as f:\n",
    "    pickle.dump(cell_specific_variant_names1, f)\n",
    "print(f\"Saved {len(cell_specific_variant_names1)} cell-specific variants (pred1) to: {pred1_cell_specific_output}\")\n",
    "\n",
    "# Save cell-nonspecific variant names for prediction model 1\n",
    "pred1_cell_nonspecific_output = os.path.join(output_dir, f'cell_nonspecific_variants_pred1_group_{group}.pkl')\n",
    "with open(pred1_cell_nonspecific_output, 'wb') as f:\n",
    "    pickle.dump(cell_nonspecific_variant_names1, f)\n",
    "print(f\"Saved {len(cell_nonspecific_variant_names1)} cell-nonspecific variants (pred1) to: {pred1_cell_nonspecific_output}\")\n",
    "\n",
    "# Save cell-specific variant names for prediction model 150\n",
    "pred150_cell_specific_output = os.path.join(output_dir, f'cell_specific_variants_pred150_group_{group}.pkl')\n",
    "with open(pred150_cell_specific_output, 'wb') as f:\n",
    "    pickle.dump(cell_specific_variant_names150, f)\n",
    "print(f\"Saved {len(cell_specific_variant_names150)} cell-specific variants (pred150) to: {pred150_cell_specific_output}\")\n",
    "\n",
    "# Save cell-nonspecific variant names for prediction model 150\n",
    "pred150_cell_nonspecific_output = os.path.join(output_dir, f'cell_nonspecific_variants_pred150_group_{group}.pkl')\n",
    "with open(pred150_cell_nonspecific_output, 'wb') as f:\n",
    "    pickle.dump(cell_nonspecific_variant_names150, f)\n",
    "print(f\"Saved {len(cell_nonspecific_variant_names150)} cell-nonspecific variants (pred150) to: {pred150_cell_nonspecific_output}\")\n",
    "\n",
    "# Optional: Save as text files (one variant name per line) for easier inspection\n",
    "# These are more readable but take more space than pickle files\n",
    "\n",
    "# Cell-specific variants for pred1\n",
    "pred1_cell_specific_txt = os.path.join(output_dir, f'cell_specific_variants_pred1_group_{group}.txt')\n",
    "with open(pred1_cell_specific_txt, 'w') as f:\n",
    "    for name in cell_specific_variant_names1:\n",
    "        f.write(f\"{name}\\n\")\n",
    "\n",
    "# Cell-nonspecific variants for pred1\n",
    "pred1_cell_nonspecific_txt = os.path.join(output_dir, f'cell_nonspecific_variants_pred1_group_{group}.txt')\n",
    "with open(pred1_cell_nonspecific_txt, 'w') as f:\n",
    "    for name in cell_nonspecific_variant_names1:\n",
    "        f.write(f\"{name}\\n\")\n",
    "\n",
    "# Cell-specific variants for pred150\n",
    "pred150_cell_specific_txt = os.path.join(output_dir, f'cell_specific_variants_pred150_group_{group}.txt')\n",
    "with open(pred150_cell_specific_txt, 'w') as f:\n",
    "    for name in cell_specific_variant_names150:\n",
    "        f.write(f\"{name}\\n\")\n",
    "\n",
    "# Cell-nonspecific variants for pred150\n",
    "pred150_cell_nonspecific_txt = os.path.join(output_dir, f'cell_nonspecific_variants_pred150_group_{group}.txt')\n",
    "with open(pred150_cell_nonspecific_txt, 'w') as f:\n",
    "    for name in cell_nonspecific_variant_names150:\n",
    "        f.write(f\"{name}\\n\")\n",
    "\n",
    "print(\"\\nAlso saved all variant names as text files for easier inspection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1035c7",
   "metadata": {},
   "source": [
    "## Loading Saved Variant Names\n",
    "\n",
    "Here's how to load the saved variant names in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce94e149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Loading the saved variant names\n",
    "def load_variant_names(file_path):\n",
    "    \"\"\"Load variant names from a pickle file.\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        variant_names = pickle.load(f)\n",
    "    return variant_names\n",
    "\n",
    "# Example usage (uncomment when needed):\n",
    "'''\n",
    "# Define the file paths\n",
    "group = 1  # Set the appropriate group number\n",
    "output_dir = os.path.join(os.path.dirname(os.path.abspath('__file__')), 'outputs')\n",
    "\n",
    "# Load cell-specific variants for pred1\n",
    "pred1_cell_specific_file = os.path.join(output_dir, f'cell_specific_variants_pred1_group_{group}.pkl')\n",
    "cell_specific_variants_pred1 = load_variant_names(pred1_cell_specific_file)\n",
    "print(f\"Loaded {len(cell_specific_variants_pred1)} cell-specific variants for pred1\")\n",
    "\n",
    "# Load cell-nonspecific variants for pred1\n",
    "pred1_cell_nonspecific_file = os.path.join(output_dir, f'cell_nonspecific_variants_pred1_group_{group}.pkl')\n",
    "cell_nonspecific_variants_pred1 = load_variant_names(pred1_cell_nonspecific_file)\n",
    "print(f\"Loaded {len(cell_nonspecific_variants_pred1)} cell-nonspecific variants for pred1\")\n",
    "\n",
    "# Load cell-specific variants for pred150\n",
    "pred150_cell_specific_file = os.path.join(output_dir, f'cell_specific_variants_pred150_group_{group}.pkl')\n",
    "cell_specific_variants_pred150 = load_variant_names(pred150_cell_specific_file)\n",
    "print(f\"Loaded {len(cell_specific_variants_pred150)} cell-specific variants for pred150\")\n",
    "\n",
    "# Load cell-nonspecific variants for pred150\n",
    "pred150_cell_nonspecific_file = os.path.join(output_dir, f'cell_nonspecific_variants_pred150_group_{group}.pkl')\n",
    "cell_nonspecific_variants_pred150 = load_variant_names(pred150_cell_nonspecific_file)\n",
    "print(f\"Loaded {len(cell_nonspecific_variants_pred150)} cell-nonspecific variants for pred150\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
