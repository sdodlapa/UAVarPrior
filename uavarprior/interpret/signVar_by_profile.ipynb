{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f864190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os.path\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0772f5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group = 1\n",
    "inPath = f'/home/sdodl001/Desktop/DNA_Methylation_Scripts/cpg_util_scripts/data/kmeans/uncert_gve_direction/{group}/pred200_merged/'\n",
    "files = os.listdir(inPath)\n",
    "files.sort()\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f90e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/148 files. Current unique names: 3645640\n",
      "Processed 20/148 files. Current unique names: 4195255\n",
      "Processed 30/148 files. Current unique names: 4554064\n",
      "Processed 40/148 files. Current unique names: 4695465\n",
      "Processed 50/148 files. Current unique names: 4931860\n",
      "Processed 60/148 files. Current unique names: 5099470\n",
      "Processed 70/148 files. Current unique names: 5138015\n",
      "Processed 80/148 files. Current unique names: 5149679\n",
      "Processed 90/148 files. Current unique names: 5257657\n",
      "Processed 100/148 files. Current unique names: 7958108\n",
      "Processed 110/148 files. Current unique names: 8837202\n",
      "Processed 120/148 files. Current unique names: 9015023\n",
      "Processed 130/148 files. Current unique names: 9120615\n",
      "Processed 140/148 files. Current unique names: 10246896\n",
      "Processed 148/148 files. Current unique names: 10358075\n",
      "\n",
      "Total unique names collected: 10358075\n",
      "Memory used by unique_names set: 114.34 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rs567865040', 'rs1426319946', 'rs927581744', 'rs1038629972', 'rs928388916']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty set to store unique name values\n",
    "unique_names = set()\n",
    "\n",
    "# Track progress\n",
    "total_files = len(files)\n",
    "processed = 0\n",
    "\n",
    "# Process files in batches to avoid memory issues\n",
    "for file in files:\n",
    "    try:\n",
    "        # Only load the 'name' column to minimize memory usage\n",
    "        data = pd.read_parquet(inPath + file, columns=['name'])\n",
    "        \n",
    "        # Update the set with new unique values\n",
    "        unique_names.update(data['name'])\n",
    "        \n",
    "        # Update and display progress\n",
    "        processed += 1\n",
    "        if processed % 10 == 0 or processed == total_files:\n",
    "            print(f\"Processed {processed}/{total_files} files. Current unique names: {len(unique_names)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nTotal unique names collected: {len(unique_names)}\")\n",
    "print(f\"Memory used by unique_names set: {sum(len(name) for name in unique_names) / (1024*1024):.2f} MB\")\n",
    "\n",
    "\n",
    "# Save the unique_names to a file\n",
    "\n",
    "# Create outputs directory if it doesn't exist (this won't be tracked by Git)\n",
    "import os\n",
    "output_dir = os.path.join(os.path.dirname(os.path.abspath('__file__')), 'outputs')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Saving large files to {output_dir} (not tracked by Git)\")\n",
    "\n",
    "# Save as pickle (Python's binary format - preserves the set data structure)\n",
    "output_path = os.path.join(output_dir, f'unique_names_group_{group}.pkl')\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(unique_names, f)\n",
    "print(f\"Saved unique names to: {output_path}\")\n",
    "print(\"WARNING: This file is large and should not be committed to Git.\")\n",
    "\n",
    "# # Optionally save as text file (one name per line)\n",
    "# text_output_path = os.path.join(output_dir, f'unique_names_group_{group}.txt')\n",
    "# with open(text_output_path, 'w') as f:\n",
    "#     for name in sorted(unique_names):\n",
    "#         f.write(f\"{name}\\n\")\n",
    "# print(f\"Saved unique names to: {text_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d65b45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3182456c",
   "metadata": {},
   "source": [
    "# Loading Saved Unique Names\n",
    "\n",
    "If you have previously saved the unique names to a file, you can load them instead of recreating the set. This is useful for:\n",
    "- Separating the data collection and analysis steps\n",
    "- Rerunning analyses with the same variant set\n",
    "- Sharing variant sets between different analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1565e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this section if you want to load previously saved unique names\n",
    "\n",
    "# Path to the saved unique names file\n",
    "unique_names_file = f'unique_names_group_{group}.pkl'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.isfile(unique_names_file):\n",
    "    # Load unique names from pickle file\n",
    "    with open(unique_names_file, 'rb') as f:\n",
    "        unique_names = pickle.load(f)\n",
    "    print(f\"Loaded {len(unique_names)} unique names from {unique_names_file}\")\n",
    "    print(f\"Memory used by unique_names set: {sum(len(name) for name in unique_names) / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    # Preview some unique names\n",
    "    print(\"\\nSample names:\")\n",
    "    print(list(unique_names)[:5])\n",
    "else:\n",
    "    print(f\"File {unique_names_file} not found. Please run the cell that creates and saves unique_names first.\")\n",
    "\n",
    "\n",
    "# Alternatively, you can load from text file if you don't have the pickle file\n",
    "'''\n",
    "unique_names_file = f'unique_names_group_{group}.txt'\n",
    "\n",
    "if os.path.isfile(unique_names_file):\n",
    "    # Load unique names from text file\n",
    "    unique_names = set()\n",
    "    with open(unique_names_file, 'r') as f:\n",
    "        for line in f:\n",
    "            unique_names.add(line.strip())\n",
    "            \n",
    "    print(f\"Loaded {len(unique_names)} unique names from {unique_names_file}\")\n",
    "else:\n",
    "    print(f\"File {unique_names_file} not found. Please run the cell that creates and saves unique_names first.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbbd3c4",
   "metadata": {},
   "source": [
    "# Creating a Binary Membership Matrix\n",
    "\n",
    "Now we'll create a binary matrix where:\n",
    "- Each row corresponds to a unique variant name\n",
    "- Each column corresponds to a file\n",
    "- Cell values are 1 (variant present in file) or 0 (variant absent)\n",
    "\n",
    "This implementation uses:\n",
    "1. A dictionary for fast lookups of variant indices\n",
    "2. Sparse matrix construction for memory efficiency\n",
    "3. Batch processing to manage memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f842838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of variant names to row indices\n",
    "start_time = time.time()\n",
    "name_to_idx = {name: idx for idx, name in enumerate(unique_names)}\n",
    "print(f\"Created name to index mapping in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Initialize lists to store the sparse matrix coordinates and values\n",
    "row_indices = []\n",
    "col_indices = []\n",
    "data_values = []\n",
    "\n",
    "# Process files to build the membership matrix\n",
    "start_time = time.time()\n",
    "total_files = len(files)\n",
    "processed = 0\n",
    "\n",
    "for file_idx, file in enumerate(files):\n",
    "    try:\n",
    "        # Only load the 'name' column\n",
    "        file_data = pd.read_parquet(inPath + file, columns=['name'])\n",
    "        \n",
    "        # Get unique names in this file (we only need each name once per file)\n",
    "        file_names = set(file_data['name'])\n",
    "        \n",
    "        # For each name in this file, add a 1 to the matrix\n",
    "        for name in file_names:\n",
    "            if name in name_to_idx:  # This should always be true but checking to be safe\n",
    "                row_indices.append(name_to_idx[name])\n",
    "                col_indices.append(file_idx)\n",
    "                data_values.append(1)\n",
    "        \n",
    "        # Update progress\n",
    "        processed += 1\n",
    "        if processed % 10 == 0 or processed == total_files:\n",
    "            print(f\"Processed {processed}/{total_files} files for matrix construction\")\n",
    "            print(f\"Current non-zero elements: {len(data_values)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file} for matrix: {str(e)}\")\n",
    "\n",
    "# Create a sparse matrix in CSR format (efficient for row operations)\n",
    "num_variants = len(unique_names)\n",
    "num_files = len(files)\n",
    "\n",
    "membership_matrix = sparse.csr_matrix(\n",
    "    (data_values, (row_indices, col_indices)),\n",
    "    shape=(num_variants, num_files)\n",
    ")\n",
    "\n",
    "print(f\"\\nMatrix shape: {membership_matrix.shape} (variants Ã— files)\")\n",
    "print(f\"Number of non-zero elements: {membership_matrix.count_nonzero()} (variant occurrences)\")\n",
    "print(f\"Sparsity: {100 - 100 * membership_matrix.count_nonzero() / (num_variants * num_files):.2f}%\")\n",
    "print(f\"Memory usage: {membership_matrix.data.nbytes / 1024**2:.2f} MB (data)\")\n",
    "print(f\"Total construction time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66603c2e",
   "metadata": {},
   "source": [
    "# Analyzing the Membership Matrix\n",
    "\n",
    "With the membership matrix constructed, you can perform various analyses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570641f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Count variants per file (column sums)\n",
    "file_variant_counts = membership_matrix.sum(axis=0).A1  # A1 converts to 1D array\n",
    "\n",
    "# Show files with most and least variants\n",
    "print(f\"Average variants per file: {file_variant_counts.mean():.2f}\")\n",
    "print(f\"Max variants in a file: {file_variant_counts.max()} (file #{file_variant_counts.argmax()}: {files[file_variant_counts.argmax()]})\")\n",
    "print(f\"Min variants in a file: {file_variant_counts.min()} (file #{file_variant_counts.argmin()}: {files[file_variant_counts.argmin()]})\")\n",
    "\n",
    "# Example 2: Count how many files each variant appears in (row sums)\n",
    "variant_file_counts = membership_matrix.sum(axis=1).A1\n",
    "\n",
    "# Show distribution of variant occurrence\n",
    "from collections import Counter\n",
    "occurrence_dist = Counter(variant_file_counts)\n",
    "\n",
    "# Print top 10 most common occurrence counts\n",
    "print(\"\\nVariant occurrence distribution:\")\n",
    "for count, num_variants in sorted(occurrence_dist.items())[:10]:\n",
    "    print(f\"{num_variants} variants appear in exactly {int(count)} files\")\n",
    "\n",
    "# Example 3: Find variants that appear in all files\n",
    "universal_variants = np.where(variant_file_counts == len(files))[0]\n",
    "print(f\"\\nNumber of variants appearing in all {len(files)} files: {len(universal_variants)}\")\n",
    "\n",
    "# If you need to get the names of specific variants (e.g., universal ones)\n",
    "if len(universal_variants) > 0:\n",
    "    idx_to_name = {idx: name for name, idx in name_to_idx.items()}\n",
    "    universal_variant_names = [idx_to_name[idx] for idx in universal_variants[:5]]  # Show first 5\n",
    "    print(f\"Sample universal variants: {universal_variant_names}\")\n",
    "\n",
    "# Example 4: Save the matrix for future use (if needed)\n",
    "\n",
    "# Save the sparse matrix\n",
    "import pickle\n",
    "with open(f'variant_membership_matrix_group_{group}.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'matrix': membership_matrix,\n",
    "        'variant_names': list(name_to_idx.keys()), \n",
    "        'file_names': files\n",
    "    }, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fugep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
